# Robots_Finder
A simple tool to quickly locate and retrieve robots.txt files from websites. Useful for ethical hacking, SEO analysis, and web development, this tool helps users understand which parts of a website are accessible to web crawlers. Supports batch processing and customizable user agents for effective scanning.

# Features

- Retrieve historical robots.txt files from Archive.org.
- Display previously disallowed paths and directories.
- Save search results to a file.
- Run the tool without console output (silent mode).
- Speed up searches with adjustable multi-threading.
